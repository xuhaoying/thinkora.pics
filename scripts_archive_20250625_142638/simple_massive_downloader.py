#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¤§è§„æ¨¡Pixabayå›¾ç‰‡ä¸‹è½½å™¨
ä½¿ç”¨å·¥ä½œçš„APIå¯†é’¥ä¸‹è½½1000å¼ å¤šæ ·åŒ–å›¾ç‰‡
"""

import os
import json
import requests
import logging
import time
import random
from datetime import datetime
from typing import List, Dict, Any
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/simple_download_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SimpleMassiveDownloader:
    def __init__(self):
        # ä½¿ç”¨æµ‹è¯•éªŒè¯è¿‡çš„APIå¯†é’¥
        self.api_key = "51008780-20fe13a52bde3f3efd30b126a"
        self.base_url = "https://pixabay.com/api/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ThinkOraPics/1.0 (https://thinkora.pics)'
        })
        
        # åˆ›å»ºç›®å½•
        os.makedirs('raw/pixabay_massive', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        
        # ä¸‹è½½ç»Ÿè®¡
        self.downloaded_ids = set()
        self.category_stats = defaultdict(int)
        self.total_downloaded = 0
        
    def get_diverse_search_plan(self) -> List[Dict[str, Any]]:
        """èŽ·å–å¤šæ ·åŒ–çš„æœç´¢è®¡åˆ’"""
        return [
            # å•†åŠ¡åŠžå…¬ (150å¼ )
            {'queries': ['office', 'business', 'laptop', 'documents', 'meeting'], 'target': 150, 'category': 'business'},
            
            # ç§‘æŠ€ç”µå­ (120å¼ )
            {'queries': ['computer', 'smartphone', 'technology', 'gadget', 'digital'], 'target': 120, 'category': 'technology'},
            
            # é£Ÿç‰©é¥®å“ (100å¼ )
            {'queries': ['food', 'coffee', 'fruit', 'vegetables', 'cooking'], 'target': 100, 'category': 'food'},
            
            # å¥åº·åŒ»ç–— (80å¼ )
            {'queries': ['health', 'medical', 'fitness', 'wellness', 'healthcare'], 'target': 80, 'category': 'health'},
            
            # æ•™è‚²å­¦ä¹  (80å¼ )
            {'queries': ['education', 'books', 'school', 'learning', 'study'], 'target': 80, 'category': 'education'},
            
            # è‡ªç„¶çŽ¯å¢ƒ (80å¼ )
            {'queries': ['nature', 'flowers', 'plants', 'trees', 'environment'], 'target': 80, 'category': 'nature'},
            
            # è¿åŠ¨å¥èº« (60å¼ )
            {'queries': ['sports', 'fitness', 'exercise', 'athletic', 'gym'], 'target': 60, 'category': 'sports'},
            
            # äº¤é€šå·¥å…· (60å¼ )
            {'queries': ['car', 'transport', 'vehicle', 'bicycle', 'automotive'], 'target': 60, 'category': 'transportation'},
            
            # æ—¶å°šé…é¥° (50å¼ )
            {'queries': ['fashion', 'jewelry', 'accessories', 'clothing', 'style'], 'target': 50, 'category': 'fashion'},
            
            # å»ºç­‘è®¾æ–½ (50å¼ )
            {'queries': ['building', 'architecture', 'construction', 'house', 'structure'], 'target': 50, 'category': 'buildings'},
            
            # éŸ³ä¹è‰ºæœ¯ (40å¼ )
            {'queries': ['music', 'instrument', 'audio', 'sound', 'entertainment'], 'target': 40, 'category': 'music'},
            
            # åŠ¨ç‰©å® ç‰© (40å¼ )
            {'queries': ['animals', 'pets', 'dog', 'cat', 'wildlife'], 'target': 40, 'category': 'animals'},
            
            # æ—…è¡Œåº¦å‡ (40å¼ )
            {'queries': ['travel', 'vacation', 'tourism', 'luggage', 'holiday'], 'target': 40, 'category': 'travel'},
            
            # å·¥ä¸šåˆ¶é€  (40å¼ )
            {'queries': ['industry', 'manufacturing', 'tools', 'equipment', 'machinery'], 'target': 40, 'category': 'industry'}
        ]
    
    def fetch_images_for_query(self, query: str, category: str, max_images: int = 50) -> List[Dict[str, Any]]:
        """ä¸ºå•ä¸ªæŸ¥è¯¢èŽ·å–å›¾ç‰‡"""
        images = []
        
        try:
            # ç®€åŒ–çš„å‚æ•°
            params = {
                'key': self.api_key,
                'q': query,
                'image_type': 'photo',
                'orientation': 'all',
                'min_width': 600,
                'min_height': 400,
                'safesearch': 'true',
                'order': 'popular',
                'per_page': min(50, max_images),
                'page': 1
            }
            
            response = self.session.get(self.base_url, params=params)
            response.raise_for_status()
            
            data = response.json()
            
            for hit in data.get('hits', []):
                if len(images) >= max_images:
                    break
                    
                image_id = str(hit['id'])
                if image_id not in self.downloaded_ids:
                    # è§£æžæ ‡ç­¾
                    tags = []
                    if hit.get('tags'):
                        tags = [tag.strip() for tag in hit['tags'].split(',')]
                    
                    image_info = {
                        'id': image_id,
                        'platform': 'pixabay',
                        'category': category,
                        'query_used': query,
                        'url': hit['largeImageURL'],
                        'download_url': hit['largeImageURL'],
                        'preview_url': hit['previewURL'],
                        'width': hit['imageWidth'],
                        'height': hit['imageHeight'],
                        'description': f"{category.title()} - {', '.join(tags[:3])}" if tags else f"{category.title()} image from Pixabay",
                        'tags': tags,
                        'author': hit['user'],
                        'author_url': f"https://pixabay.com/users/{hit['user']}-{hit['user_id']}/",
                        'views': hit.get('views', 0),
                        'downloads': hit.get('downloads', 0),
                        'likes': hit.get('likes', 0),
                        'quality_score': hit.get('likes', 0) + hit.get('downloads', 0),
                        'fetch_metadata': {
                            'category_assigned': category,
                            'query_original': query,
                            'fetch_time': datetime.now().isoformat()
                        }
                    }
                    images.append(image_info)
                    self.downloaded_ids.add(image_id)
            
            logger.info(f"Query '{query}' ({category}): found {len(images)} images")
            
        except Exception as e:
            logger.error(f"Error fetching images for query '{query}': {e}")
        
        return images
    
    def download_image(self, image_info: Dict[str, Any]) -> bool:
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            image_id = image_info['id']
            category = image_info['category']
            url = image_info['download_url']
            
            # æž„å»ºæ–‡ä»¶è·¯å¾„
            filename = f"pixabay_{category}_{image_id}.jpg"
            filepath = os.path.join('raw', 'pixabay_massive', filename)
            
            # å¦‚æžœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
            if os.path.exists(filepath):
                logger.debug(f"Image already exists: {filename}")
                return True
            
            # ä¸‹è½½å›¾ç‰‡
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # ä¿å­˜å›¾ç‰‡
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_path = filepath.replace('.jpg', '_metadata.json')
            with open(metadata_path, 'w') as f:
                json.dump(image_info, f, indent=2, ensure_ascii=False)
            
            self.total_downloaded += 1
            self.category_stats[category] += 1
            
            logger.info(f"âœ… Downloaded {self.total_downloaded}: {filename} (quality: {image_info['quality_score']})")
            
            # æŽ§åˆ¶ä¸‹è½½é¢‘çŽ‡
            time.sleep(random.uniform(0.2, 0.5))
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error downloading image {image_info['id']}: {e}")
            return False
    
    def run_massive_download(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¤§è§„æ¨¡ä¸‹è½½ä»»åŠ¡"""
        start_time = datetime.now()
        logger.info("ðŸš€ Starting simplified massive Pixabay download - Target: 1000 diverse images")
        
        search_plan = self.get_diverse_search_plan()
        all_images = []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†å›¾ç‰‡ä¿¡æ¯
        logger.info("\nðŸ“Š Phase 1: Collecting image information...")
        
        for plan in search_plan:
            if len(all_images) >= 1000:
                break
                
            category = plan['category']
            queries = plan['queries']
            target = plan['target']
            
            logger.info(f"\nðŸ“ Category: {category} (target: {target} images)")
            
            category_images = []
            images_per_query = max(1, target // len(queries))
            
            for query in queries:
                if len(category_images) >= target:
                    break
                    
                logger.info(f"  ðŸ” Searching: {query}")
                query_images = self.fetch_images_for_query(query, category, images_per_query + 10)
                category_images.extend(query_images)
                
                # APIé¢‘çŽ‡æŽ§åˆ¶
                time.sleep(0.6)
            
            # æŒ‰è´¨é‡æŽ’åºå¹¶é™åˆ¶æ•°é‡
            category_images.sort(key=lambda x: x['quality_score'], reverse=True)
            category_images = category_images[:target]
            
            all_images.extend(category_images)
            logger.info(f"âœ… Category {category}: collected {len(category_images)} images")
        
        # é™åˆ¶åˆ°1000å¼ 
        if len(all_images) > 1000:
            all_images = all_images[:1000]
        
        logger.info(f"\nðŸ“Š Collection completed: {len(all_images)} images ready for download")
        
        # ç¬¬äºŒé˜¶æ®µï¼šä¸‹è½½å›¾ç‰‡
        logger.info("\nâ¬‡ï¸ Phase 2: Downloading images...")
        
        success_count = 0
        failed_count = 0
        
        for i, image in enumerate(all_images, 1):
            logger.info(f"\nðŸ“¥ [{i}/{len(all_images)}] Downloading: {image['category']} - {image['id']}")
            
            if self.download_image(image):
                success_count += 1
            else:
                failed_count += 1
            
            # æ¯100å¼ å›¾ç‰‡è¾“å‡ºè¿›åº¦æŠ¥å‘Š
            if i % 100 == 0:
                elapsed = datetime.now() - start_time
                rate = i / elapsed.total_seconds() * 60  # æ¯åˆ†é’Ÿå¤„ç†æ•°
                eta_minutes = (len(all_images) - i) / rate if rate > 0 else 0
                
                logger.info(f"""
ðŸ”„ Progress Report - {i}/{len(all_images)} processed
âœ… Success: {success_count} | âŒ Failed: {failed_count}
â±ï¸ Speed: {rate:.1f} images/min | ETA: {eta_minutes:.1f} minutes
ðŸ“Š Categories: {dict(self.category_stats)}
                """)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        end_time = datetime.now()
        total_duration = end_time - start_time
        
        report = {
            'summary': {
                'total_processed': len(all_images),
                'successfully_downloaded': success_count,
                'failed_downloads': failed_count,
                'success_rate': f"{success_count / len(all_images) * 100:.1f}%",
                'duration': str(total_duration),
                'download_speed': f"{success_count / total_duration.total_seconds() * 60:.1f} images/min"
            },
            'category_distribution': dict(self.category_stats),
            'quality_stats': {
                'high_quality': len([img for img in all_images if img['quality_score'] > 100]),
                'medium_quality': len([img for img in all_images if 20 <= img['quality_score'] <= 100]),
                'basic_quality': len([img for img in all_images if img['quality_score'] < 20])
            },
            'resolution_stats': {
                'high_res': len([img for img in all_images if img['width'] >= 1920]),
                'medium_res': len([img for img in all_images if 1200 <= img['width'] < 1920]),
                'standard_res': len([img for img in all_images if img['width'] < 1200])
            },
            'metadata': {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'api_key_used': self.api_key[:10] + "...",
                'total_categories': len(search_plan)
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"logs/simple_massive_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"""
ðŸŽ‰ MASSIVE DOWNLOAD COMPLETED! ðŸŽ‰

ðŸ“Š Final Statistics:
â€¢ Total processed: {len(all_images)}
â€¢ Successfully downloaded: {success_count}
â€¢ Failed downloads: {failed_count}
â€¢ Success rate: {success_count / len(all_images) * 100:.1f}%
â€¢ Total duration: {total_duration}
â€¢ Download speed: {success_count / total_duration.total_seconds() * 60:.1f} images/min

ðŸ“ Category distribution:
{chr(10).join(f'â€¢ {cat}: {count} images' for cat, count in self.category_stats.items())}

ðŸ“„ Report saved to: {report_file}
        """)
        
        return report


def main():
    """ä¸»å‡½æ•°"""
    try:
        downloader = SimpleMassiveDownloader()
        report = downloader.run_massive_download()
        
        # è¾“å‡ºJSONæ ¼å¼çš„æŠ¥å‘Š
        print("\n" + "="*50)
        print("FINAL REPORT JSON:")
        print("="*50)
        print(json.dumps(report, indent=2, ensure_ascii=False))
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ Download interrupted by user")
    except Exception as e:
        logger.error(f"ðŸ’¥ Critical error: {e}")
        raise


if __name__ == "__main__":
    main()