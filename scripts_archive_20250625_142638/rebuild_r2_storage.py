#!/usr/bin/env python3
"""
é‡å»ºR2å­˜å‚¨æ¡¶ - åªä¸Šä¼ å½“å‰æ•°æ®åº“ä¸­çš„å›¾ç‰‡
"""

import os
import json
import sqlite3
import requests
import subprocess
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# R2é…ç½®
R2_BUCKET_NAME = 'thinkora-pics'
R2_PUBLIC_URL = 'https://r2.thinkora.pics'  # ä½ çš„R2å…¬å¼€URL

def get_current_images():
    """è·å–å½“å‰æ•°æ®åº“ä¸­çš„æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯"""
    conn = sqlite3.connect('thinkora.db')
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT id, title, tags, width, height, 
               url_thumbnail, url_regular, url_download
        FROM images
        ORDER BY id
    """)
    
    images = []
    for row in cursor.fetchall():
        images.append({
            'id': row[0],
            'title': row[1],
            'tags': json.loads(row[2]),
            'width': row[3],
            'height': row[4],
            'url_thumbnail': row[5],
            'url_regular': row[6],
            'url_download': row[7]
        })
    
    conn.close()
    logger.info(f"ğŸ“¸ ä»æ•°æ®åº“è·å– {len(images)} å¼ å›¾ç‰‡ä¿¡æ¯")
    return images

def find_local_image_file(image_id):
    """æŸ¥æ‰¾æœ¬åœ°å›¾ç‰‡æ–‡ä»¶"""
    # å¯èƒ½çš„ä½ç½®
    possible_paths = [
        f'raw/pixabay/{image_id}.jpg',
        f'raw/unsplash/{image_id}.jpg',
        f'raw/pexels/{image_id}.jpg',
        f'png/{image_id}.png',
        f'processed_backup/pixabay/{image_id}.jpg',
        f'processed_backup/unsplash/{image_id}.jpg',
        f'processed_backup/pexels/{image_id}.jpg',
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    
    return None

def download_image_if_needed(image):
    """å¦‚æœæœ¬åœ°æ²¡æœ‰å›¾ç‰‡ï¼Œä»URLä¸‹è½½"""
    image_id = image['id']
    local_path = find_local_image_file(image_id)
    
    if local_path:
        return local_path
    
    # éœ€è¦ä¸‹è½½
    logger.info(f"â¬‡ï¸ ä¸‹è½½å›¾ç‰‡: {image_id}")
    
    # ç¡®å®šä¿å­˜è·¯å¾„
    platform = image_id.split('_')[0]
    save_dir = f'raw/{platform}'
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, f'{image_id}.jpg')
    
    # å°è¯•ä¸‹è½½
    url = image.get('url_download') or image.get('url_regular')
    if not url:
        logger.error(f"âŒ æ²¡æœ‰ä¸‹è½½URL: {image_id}")
        return None
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        with open(save_path, 'wb') as f:
            f.write(response.content)
        
        logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {save_path}")
        return save_path
        
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½å¤±è´¥ {image_id}: {e}")
        return None

def calculate_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def upload_to_r2_rclone(local_path, r2_path):
    """ä½¿ç”¨rcloneä¸Šä¼ åˆ°R2"""
    try:
        cmd = [
            'rclone', 'copy',
            local_path,
            f'r2:{R2_BUCKET_NAME}/{os.path.dirname(r2_path)}',
            '--s3-acl', 'public-read',
            '--progress'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            return True
        else:
            logger.error(f"rcloneé”™è¯¯: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"ä¸Šä¼ å¼‚å¸¸: {e}")
        return False

def process_image(image, use_rclone=True):
    """å¤„ç†å•å¼ å›¾ç‰‡"""
    image_id = image['id']
    
    # 1. è·å–æœ¬åœ°æ–‡ä»¶
    local_path = download_image_if_needed(image)
    if not local_path:
        return {
            'id': image_id,
            'status': 'failed',
            'error': 'No local file'
        }
    
    # 2. ç¡®å®šR2è·¯å¾„
    ext = os.path.splitext(local_path)[1]
    r2_path = f'images/{image_id}{ext}'
    
    # 3. ä¸Šä¼ åˆ°R2
    if use_rclone:
        success = upload_to_r2_rclone(local_path, r2_path)
    else:
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–ä¸Šä¼ æ–¹æ³•
        success = False
    
    if success:
        # è®¡ç®—æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(local_path)
        file_hash = calculate_file_hash(local_path)
        
        return {
            'id': image_id,
            'status': 'success',
            'local_path': local_path,
            'r2_path': r2_path,
            'r2_url': f'{R2_PUBLIC_URL}/{r2_path}',
            'file_size': file_size,
            'file_hash': file_hash,
            'tags': image['tags']
        }
    else:
        return {
            'id': image_id,
            'status': 'failed',
            'error': 'Upload failed'
        }

def update_database_urls(uploaded_images):
    """æ›´æ–°æ•°æ®åº“ä¸­çš„å›¾ç‰‡URL"""
    conn = sqlite3.connect('thinkora.db')
    cursor = conn.cursor()
    
    updated_count = 0
    for img in uploaded_images:
        if img['status'] == 'success':
            r2_url = img['r2_url']
            cursor.execute("""
                UPDATE images 
                SET url_thumbnail = ?, url_regular = ?, url_download = ?
                WHERE id = ?
            """, (r2_url, r2_url, r2_url, img['id']))
            updated_count += 1
    
    conn.commit()
    conn.close()
    
    logger.info(f"âœ… æ›´æ–°äº† {updated_count} æ¡æ•°æ®åº“è®°å½•")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='é‡å»ºR2å­˜å‚¨æ¡¶')
    parser.add_argument('--check-rclone', action='store_true', help='æ£€æŸ¥rcloneé…ç½®')
    parser.add_argument('--dry-run', action='store_true', help='åªæ£€æŸ¥ï¼Œä¸å®é™…ä¸Šä¼ ')
    parser.add_argument('--workers', type=int, default=4, help='å¹¶å‘ä¸Šä¼ æ•°é‡')
    args = parser.parse_args()
    
    logger.info("ğŸš€ å¼€å§‹é‡å»ºR2å­˜å‚¨æ¡¶")
    logger.info("=" * 50)
    
    # æ£€æŸ¥rclone
    if args.check_rclone:
        try:
            result = subprocess.run(['rclone', 'version'], capture_output=True)
            logger.info("âœ… rcloneå·²å®‰è£…")
            
            # æ£€æŸ¥R2é…ç½®
            result = subprocess.run(['rclone', 'ls', f'r2:{R2_BUCKET_NAME}', '--max-depth', '1'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                logger.info("âœ… rclone R2é…ç½®æ­£å¸¸")
            else:
                logger.error("âŒ rclone R2é…ç½®é”™è¯¯ï¼Œè¯·è¿è¡Œ: rclone config")
                return
        except:
            logger.error("âŒ rcloneæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: brew install rclone")
            return
    
    # 1. è·å–å½“å‰æ•°æ®åº“ä¸­çš„å›¾ç‰‡
    images = get_current_images()
    
    if args.dry_run:
        logger.info("\nğŸ” è¯•è¿è¡Œæ¨¡å¼ - æ£€æŸ¥æœ¬åœ°æ–‡ä»¶")
        missing_count = 0
        for img in images:
            local_path = find_local_image_file(img['id'])
            if not local_path:
                logger.warning(f"ç¼ºå¤±: {img['id']}")
                missing_count += 1
        
        logger.info(f"\nğŸ“Š ç»Ÿè®¡:")
        logger.info(f"  æ€»å›¾ç‰‡: {len(images)}")
        logger.info(f"  æœ¬åœ°å­˜åœ¨: {len(images) - missing_count}")
        logger.info(f"  éœ€è¦ä¸‹è½½: {missing_count}")
        return
    
    # 2. å‡†å¤‡ä¸Šä¼ 
    logger.info(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼  {len(images)} å¼ å›¾ç‰‡åˆ°R2...")
    
    uploaded_images = []
    failed_images = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸Šä¼ 
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_image = {
            executor.submit(process_image, img): img 
            for img in images
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for i, future in enumerate(as_completed(future_to_image), 1):
            result = future.result()
            
            if result['status'] == 'success':
                uploaded_images.append(result)
                logger.info(f"[{i}/{len(images)}] âœ… {result['id']}")
            else:
                failed_images.append(result)
                logger.error(f"[{i}/{len(images)}] âŒ {result['id']}: {result.get('error')}")
    
    # 3. æ›´æ–°æ•°æ®åº“URL
    if uploaded_images:
        logger.info("\nğŸ“ æ›´æ–°æ•°æ®åº“URL...")
        update_database_urls(uploaded_images)
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    report = {
        'timestamp': datetime.now().isoformat(),
        'total_images': len(images),
        'uploaded': len(uploaded_images),
        'failed': len(failed_images),
        'total_size_mb': sum(img['file_size'] for img in uploaded_images) / 1024 / 1024,
        'uploaded_files': uploaded_images,
        'failed_files': failed_images
    }
    
    report_path = f'logs/r2_rebuild_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    # 5. æ˜¾ç¤ºæ‘˜è¦
    logger.info("\nâœ… é‡å»ºå®Œæˆ!")
    logger.info(f"ğŸ“Š ä¸Šä¼ ç»Ÿè®¡:")
    logger.info(f"  æˆåŠŸ: {len(uploaded_images)} å¼ ")
    logger.info(f"  å¤±è´¥: {len(failed_images)} å¼ ")
    logger.info(f"  æ€»å¤§å°: {report['total_size_mb']:.1f} MB")
    logger.info(f"  è¯¦ç»†æŠ¥å‘Š: {report_path}")
    
    if failed_images:
        logger.info(f"\nâš ï¸ å¤±è´¥çš„å›¾ç‰‡ID:")
        for img in failed_images[:10]:
            logger.info(f"  - {img['id']}: {img.get('error')}")

if __name__ == '__main__':
    main()