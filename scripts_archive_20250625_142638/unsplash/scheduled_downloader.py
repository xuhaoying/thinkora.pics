import os
import json
import requests
import time
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import Dict, List, Set
import argparse
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('download.log'),
        logging.StreamHandler()
    ]
)

@dataclass
class DownloadConfig:
    """ä¸‹è½½é…ç½®"""
    max_requests_per_hour: int = 45
    batch_size: int = 15  # æ¯æ¬¡ä¸‹è½½çš„å›¾ç‰‡æ•°
    min_resolution: int = 1000  # æœ€å°åˆ†è¾¨ç‡
    max_file_size: int = 5 * 1024 * 1024  # 5MB
    quality_threshold: float = 0.7  # è´¨é‡é˜ˆå€¼

class ImageDownloader:
    def __init__(self, config: DownloadConfig):
        self.config = config
        self.access_key = os.getenv("ACCESS_KEY")
        self.state_file = "download_state.json"
        self.metadata_file = "metadata.json"
        self.downloaded_ids_file = "downloaded_ids.json"
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.load_state()
        self.load_downloaded_ids()
        
        # æŸ¥è¯¢é˜Ÿåˆ—ï¼ˆè½®æ¢ä½¿ç”¨ï¼‰
        self.query_queue = [
            {"query": "laptop computer minimal", "priority": 10},
            {"query": "smartphone mobile device", "priority": 9},
            {"query": "coffee cup workspace", "priority": 8},
            {"query": "office supplies desk", "priority": 7},
            {"query": "plant succulent decoration", "priority": 6},
            {"query": "book notebook reading", "priority": 5},
            {"query": "camera photography equipment", "priority": 4},
            {"query": "headphones audio technology", "priority": 3},
            {"query": "watch clock time", "priority": 2},
            {"query": "keyboard mouse tech", "priority": 1},
        ]
        
    def load_state(self):
        """åŠ è½½ä¸‹è½½çŠ¶æ€"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r') as f:
                self.state = json.load(f)
        else:
            self.state = {
                "last_run": None,
                "current_query_index": 0,
                "total_downloaded": 0,
                "last_hour_requests": 0,
                "last_hour_start": None,
                "failed_downloads": [],
                "query_stats": {}
            }
    
    def save_state(self):
        """ä¿å­˜ä¸‹è½½çŠ¶æ€"""
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2)
    
    def load_downloaded_ids(self):
        """åŠ è½½å·²ä¸‹è½½å›¾ç‰‡IDåˆ—è¡¨"""
        if os.path.exists(self.downloaded_ids_file):
            with open(self.downloaded_ids_file, 'r') as f:
                self.downloaded_ids = set(json.load(f))
        else:
            self.downloaded_ids = set()
        
        logging.info(f"å·²åŠ è½½ {len(self.downloaded_ids)} ä¸ªå·²ä¸‹è½½å›¾ç‰‡ID")
    
    def save_downloaded_ids(self):
        """ä¿å­˜å·²ä¸‹è½½å›¾ç‰‡IDåˆ—è¡¨"""
        with open(self.downloaded_ids_file, 'w') as f:
            json.dump(list(self.downloaded_ids), f)
    
    def load_metadata(self):
        """åŠ è½½ç°æœ‰å…ƒæ•°æ®"""
        if os.path.exists(self.metadata_file):
            with open(self.metadata_file, 'r') as f:
                return json.load(f)
        return {}
    
    def save_metadata(self, metadata):
        """ä¿å­˜å…ƒæ•°æ®"""
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def can_make_requests(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­å‘é€è¯·æ±‚"""
        now = datetime.now()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„ä¸€å°æ—¶
        if self.state["last_hour_start"]:
            last_hour = datetime.fromisoformat(self.state["last_hour_start"])
            if now - last_hour >= timedelta(hours=1):
                # é‡ç½®è®¡æ•°å™¨
                self.state["last_hour_requests"] = 0
                self.state["last_hour_start"] = now.isoformat()
        else:
            self.state["last_hour_start"] = now.isoformat()
        
        return self.state["last_hour_requests"] < self.config.max_requests_per_hour
    
    def calculate_image_hash(self, img_data: bytes) -> str:
        """è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        return hashlib.md5(img_data).hexdigest()
    
    def is_high_quality_image(self, photo: dict) -> bool:
        """åˆ¤æ–­å›¾ç‰‡æ˜¯å¦ç¬¦åˆè´¨é‡è¦æ±‚"""
        # æ£€æŸ¥åˆ†è¾¨ç‡
        if photo["width"] < self.config.min_resolution or photo["height"] < self.config.min_resolution:
            return False
        
        # æ£€æŸ¥å®½é«˜æ¯”ï¼ˆé¿å…è¿‡äºæç«¯çš„æ¯”ä¾‹ï¼‰
        ratio = max(photo["width"], photo["height"]) / min(photo["width"], photo["height"])
        if ratio > 5:  # å®½é«˜æ¯”è¶…è¿‡5:1
            return False
        
        # æ£€æŸ¥æè¿°è´¨é‡ï¼ˆæœ‰æè¿°çš„é€šå¸¸è´¨é‡æ›´å¥½ï¼‰
        has_description = bool(photo.get("description") or photo.get("alt_description"))
        
        # æ£€æŸ¥ä½œè€…æ´»è·ƒåº¦ï¼ˆä¸‹è½½é‡é«˜çš„ä½œè€…é€šå¸¸è´¨é‡æ›´å¥½ï¼‰
        author_downloads = photo["user"].get("total_photos", 0)
        
        # ç»¼åˆè¯„åˆ†
        quality_score = 0.5  # åŸºç¡€åˆ†
        if has_description:
            quality_score += 0.2
        if author_downloads > 100:
            quality_score += 0.2
        if photo["likes"] > 50:
            quality_score += 0.1
        
        return quality_score >= self.config.quality_threshold
    
    def get_next_query(self) -> dict:
        """è·å–ä¸‹ä¸€ä¸ªæŸ¥è¯¢"""
        query_info = self.query_queue[self.state["current_query_index"]]
        
        # æ›´æ–°ç´¢å¼•ï¼ˆå¾ªç¯ï¼‰
        self.state["current_query_index"] = (self.state["current_query_index"] + 1) % len(self.query_queue)
        
        return query_info
    
    def download_batch(self) -> int:
        """ä¸‹è½½ä¸€æ‰¹å›¾ç‰‡"""
        if not self.can_make_requests():
            logging.warning("å·²è¾¾åˆ°APIè¯·æ±‚é™åˆ¶ï¼Œè·³è¿‡æœ¬æ¬¡ä¸‹è½½")
            return 0
        
        query_info = self.get_next_query()
        query = query_info["query"]
        
        logging.info(f"å¼€å§‹ä¸‹è½½æ‰¹æ¬¡: '{query}'")
        
        # åˆ›å»ºç›®å½•
        Path("raw").mkdir(exist_ok=True)
        
        # åŠ è½½ç°æœ‰å…ƒæ•°æ®
        metadata = self.load_metadata()
        
        downloaded_count = 0
        page = 1
        
        while downloaded_count < self.config.batch_size and self.can_make_requests():
            try:
                # æœç´¢è¯·æ±‚
                response = requests.get(
                    "https://api.unsplash.com/search/photos",
                    params={
                        "query": query,
                        "per_page": 30,
                        "page": page,
                        "order_by": "relevant"
                    },
                    headers={"Authorization": f"Client-ID {self.access_key}"},
                    timeout=10
                )
                response.raise_for_status()
                self.state["last_hour_requests"] += 1
                
                results = response.json()["results"]
                if not results:
                    logging.info(f"æŸ¥è¯¢ '{query}' ç¬¬ {page} é¡µæ— æ›´å¤šç»“æœ")
                    break
                
                for photo in results:
                    if downloaded_count >= self.config.batch_size:
                        break
                    
                    # å¤šå±‚å»é‡æ£€æŸ¥
                    if self.is_duplicate(photo):
                        continue
                    
                    # è´¨é‡æ£€æŸ¥
                    if not self.is_high_quality_image(photo):
                        logging.debug(f"è·³è¿‡ä½è´¨é‡å›¾ç‰‡: {photo['id']}")
                        continue
                    
                    # ä¸‹è½½å›¾ç‰‡
                    success = self.download_single_image(photo, metadata)
                    if success:
                        downloaded_count += 1
                        self.downloaded_ids.add(photo["id"])
                        logging.info(f"æˆåŠŸä¸‹è½½: {photo['id']} ({downloaded_count}/{self.config.batch_size})")
                    
                    if not self.can_make_requests():
                        logging.warning("APIè¯·æ±‚è¾¾åˆ°é™åˆ¶ï¼Œåœæ­¢ä¸‹è½½")
                        break
                
                page += 1
                
            except Exception as e:
                logging.error(f"æœç´¢è¯·æ±‚å¤±è´¥: {e}")
                break
        
        # æ›´æ–°ç»Ÿè®¡
        if query not in self.state["query_stats"]:
            self.state["query_stats"][query] = {"downloaded": 0, "last_run": None}
        
        self.state["query_stats"][query]["downloaded"] += downloaded_count
        self.state["query_stats"][query]["last_run"] = datetime.now().isoformat()
        self.state["total_downloaded"] += downloaded_count
        
        # ä¿å­˜çŠ¶æ€å’Œæ•°æ®
        self.save_state()
        self.save_downloaded_ids()
        self.save_metadata(metadata)
        
        logging.info(f"æ‰¹æ¬¡å®Œæˆ: ä¸‹è½½äº† {downloaded_count} å¼ å›¾ç‰‡")
        return downloaded_count
    
    def is_duplicate(self, photo: dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å›¾ç‰‡"""
        # 1. IDå»é‡
        if photo["id"] in self.downloaded_ids:
            return True
        
        # 2. URLå»é‡ï¼ˆæ£€æŸ¥åŸå§‹URLï¼‰
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„é‡å¤æ£€æµ‹é€»è¾‘
        
        return False
    
    def download_single_image(self, photo: dict, metadata: dict) -> bool:
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            # ä¸‹è½½å›¾ç‰‡
            img_response = requests.get(photo["urls"]["regular"], timeout=15)
            img_response.raise_for_status()
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if len(img_response.content) > self.config.max_file_size:
                logging.warning(f"å›¾ç‰‡ {photo['id']} è¿‡å¤§ï¼Œè·³è¿‡")
                return False
            
            # ä¿å­˜æ–‡ä»¶
            raw_path = f"raw/{photo['id']}.jpg"
            with open(raw_path, "wb") as f:
                f.write(img_response.content)
            
            # å‘é€ä¸‹è½½ç»Ÿè®¡è¯·æ±‚ï¼ˆå¿…éœ€ï¼‰
            if self.can_make_requests():
                requests.get(
                    photo["links"]["download_location"],
                    headers={"Authorization": f"Client-ID {self.access_key}"},
                    timeout=5
                )
                self.state["last_hour_requests"] += 1
            
            # ç”Ÿæˆå…ƒæ•°æ®
            metadata[photo["id"]] = {
                "id": photo["id"],
                "title": photo.get("description") or photo.get("alt_description") or f"Image {photo['id']}",
                "author": photo["user"]["name"],
                "author_url": photo["user"]["links"]["html"],
                "description": photo.get("description", ""),
                "width": photo["width"],
                "height": photo["height"],
                "likes": photo.get("likes", 0),
                "unsplash_url": photo["links"]["html"],
                "download_url": photo["urls"]["regular"],
                "raw_path": raw_path,
                "downloaded_at": datetime.now().isoformat(),
                "file_size": len(img_response.content),
                "file_hash": self.calculate_image_hash(img_response.content)
            }
            
            return True
            
        except Exception as e:
            logging.error(f"ä¸‹è½½å›¾ç‰‡ {photo['id']} å¤±è´¥: {e}")
            self.state["failed_downloads"].append({
                "id": photo["id"],
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            return False
    
    def run_continuous(self, hours: int = 24):
        """è¿ç»­è¿è¡ŒæŒ‡å®šå°æ—¶æ•°"""
        end_time = datetime.now() + timedelta(hours=hours)
        
        logging.info(f"å¼€å§‹è¿ç»­è¿è¡Œ {hours} å°æ—¶")
        
        while datetime.now() < end_time:
            try:
                if self.can_make_requests():
                    downloaded = self.download_batch()
                    if downloaded > 0:
                        logging.info(f"æœ¬æ¬¡ä¸‹è½½ {downloaded} å¼ ï¼Œæ€»è®¡ {self.state['total_downloaded']} å¼ ")
                else:
                    # ç­‰å¾…åˆ°ä¸‹ä¸€å°æ—¶
                    if self.state["last_hour_start"]:
                        last_hour = datetime.fromisoformat(self.state["last_hour_start"])
                        wait_time = 3600 - (datetime.now() - last_hour).total_seconds()
                        if wait_time > 0:
                            logging.info(f"ç­‰å¾… {wait_time/60:.1f} åˆ†é’Ÿåˆ°ä¸‹ä¸€å°æ—¶")
                            time.sleep(min(wait_time + 60, 300))  # æœ€å¤šç­‰5åˆ†é’Ÿ
                
                # æ¯æ¬¡é—´éš”5-10åˆ†é’Ÿ
                time.sleep(300 + (hash(str(datetime.now())) % 300))
                
            except KeyboardInterrupt:
                logging.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
                break
            except Exception as e:
                logging.error(f"è¿è¡Œå‡ºé”™: {e}")
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿ
        
        logging.info("è¿ç»­è¿è¡Œç»“æŸ")
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        print(f"æ€»ä¸‹è½½æ•°: {self.state['total_downloaded']}")
        print(f"æœ¬å°æ—¶è¯·æ±‚æ•°: {self.state['last_hour_requests']}/{self.config.max_requests_per_hour}")
        print(f"å¤±è´¥ä¸‹è½½æ•°: {len(self.state['failed_downloads'])}")
        
        print("\nğŸ“ˆ æŸ¥è¯¢ç»Ÿè®¡:")
        for query, stats in self.state["query_stats"].items():
            print(f"  {query}: {stats['downloaded']} å¼ ")

def main():
    parser = argparse.ArgumentParser(description="å®šæ—¶å›¾ç‰‡ä¸‹è½½å™¨")
    parser.add_argument("--mode", choices=["single", "continuous"], default="single",
                       help="è¿è¡Œæ¨¡å¼ï¼šsingle=å•æ¬¡ï¼Œcontinuous=è¿ç»­")
    parser.add_argument("--hours", type=int, default=24,
                       help="è¿ç»­æ¨¡å¼è¿è¡Œå°æ—¶æ•°")
    parser.add_argument("--batch-size", type=int, default=15,
                       help="æ¯æ‰¹ä¸‹è½½å›¾ç‰‡æ•°")
    args = parser.parse_args()
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("ACCESS_KEY"):
        print("âŒ è¯·è®¾ç½® ACCESS_KEY ç¯å¢ƒå˜é‡")
        return
    
    # åˆ›å»ºé…ç½®
    config = DownloadConfig(batch_size=args.batch_size)
    downloader = ImageDownloader(config)
    
    # è¿è¡Œ
    if args.mode == "single":
        downloaded = downloader.download_batch()
        print(f"âœ… å•æ¬¡è¿è¡Œå®Œæˆï¼Œä¸‹è½½äº† {downloaded} å¼ å›¾ç‰‡")
    else:
        downloader.run_continuous(args.hours)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    downloader.print_stats()

if __name__ == "__main__":
    main()