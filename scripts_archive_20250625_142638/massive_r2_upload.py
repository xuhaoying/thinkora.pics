#!/usr/bin/env python3
"""
å¤§æ‰¹é‡R2ä¸Šä¼ å™¨
å°†å¤„ç†åçš„é€æ˜èƒŒæ™¯PNGå›¾ç‰‡ä¸Šä¼ åˆ°Cloudflare R2
"""

import os
import json
import boto3
import logging
import time
from datetime import datetime
from typing import List, Dict, Any
from collections import defaultdict
import glob
from botocore.client import Config
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env') or load_dotenv('unsplash/.env')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/r2_upload_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MassiveR2Uploader:
    def __init__(self):
        self.input_dir = 'png/pixabay_massive'
        
        # R2é…ç½®ä»ç¯å¢ƒå˜é‡è·å–
        self.r2_endpoint = os.getenv('R2_ENDPOINT')
        self.r2_access_key = os.getenv('R2_ACCESS_KEY_ID')
        self.r2_secret_key = os.getenv('R2_SECRET_ACCESS_KEY')
        self.r2_bucket = os.getenv('R2_BUCKET')
        self.r2_public_url = os.getenv('R2_PUBLIC_URL', 'https://img.thinkora.pics')
        
        # éªŒè¯é…ç½®
        if not all([self.r2_endpoint, self.r2_access_key, self.r2_secret_key, self.r2_bucket]):
            raise ValueError("Missing R2 configuration. Check environment variables.")
        
        # åˆå§‹åŒ–R2å®¢æˆ·ç«¯
        self.r2_client = boto3.client(
            's3',
            endpoint_url=self.r2_endpoint,
            aws_access_key_id=self.r2_access_key,
            aws_secret_access_key=self.r2_secret_key,
            config=Config(signature_version='s3v4')
        )
        
        # åˆ›å»ºç›®å½•
        os.makedirs('logs', exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_found': 0,
            'uploaded': 0,
            'failed': 0,
            'skipped': 0,
            'category_stats': defaultdict(int),
            'start_time': datetime.now()
        }
        
        logger.info(f"âœ… R2 uploader initialized")
        logger.info(f"ğŸ”— Endpoint: {self.r2_endpoint}")
        logger.info(f"ğŸª£ Bucket: {self.r2_bucket}")
        logger.info(f"ğŸŒ Public URL: {self.r2_public_url}")
    
    def get_png_files(self) -> List[str]:
        """è·å–æ‰€æœ‰éœ€è¦ä¸Šä¼ çš„PNGæ–‡ä»¶"""
        pattern = os.path.join(self.input_dir, "*.png")
        files = glob.glob(pattern)
        
        logger.info(f"ğŸ“ Found {len(files)} PNG files to upload")
        self.stats['total_found'] = len(files)
        
        return sorted(files)
    
    def load_image_metadata(self, png_path: str) -> Dict[str, Any]:
        """åŠ è½½å›¾ç‰‡å…ƒæ•°æ®"""
        try:
            metadata_path = png_path.replace('.png', '_metadata.json')
            
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                logger.warning(f"No metadata found for {os.path.basename(png_path)}")
                
                # ä»æ–‡ä»¶åè§£æåŸºæœ¬ä¿¡æ¯
                filename = os.path.basename(png_path)
                name_without_ext = os.path.splitext(filename)[0]
                parts = name_without_ext.split('_')
                
                return {
                    'id': parts[2] if len(parts) > 2 else name_without_ext,
                    'category': parts[1] if len(parts) > 1 else 'unknown',
                    'title': f"Image {parts[2] if len(parts) > 2 else name_without_ext}",
                    'platform': 'pixabay',
                    'background_removed': True,
                    'file_format': 'PNG'
                }
                
        except Exception as e:
            logger.error(f"Error loading metadata for {png_path}: {e}")
            return {
                'id': os.path.splitext(os.path.basename(png_path))[0],
                'category': 'unknown',
                'title': 'Unknown image',
                'platform': 'pixabay',
                'background_removed': True,
                'file_format': 'PNG'
            }
    
    def check_if_exists(self, s3_key: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²åœ¨R2ä¸­å­˜åœ¨"""
        try:
            self.r2_client.head_object(Bucket=self.r2_bucket, Key=s3_key)
            return True
        except:
            return False
    
    def upload_single_file(self, local_path: str, metadata: Dict[str, Any]) -> bool:
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ°R2"""
        try:
            filename = os.path.basename(local_path)
            s3_key = f"images/{filename}"
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if self.check_if_exists(s3_key):
                logger.debug(f"â­ï¸ File already exists in R2: {filename}")
                self.stats['skipped'] += 1
                return True
            
            # å‡†å¤‡ä¸Šä¼ å…ƒæ•°æ®
            upload_metadata = {
                'Content-Type': 'image/png',
                'Cache-Control': 'public, max-age=31536000',  # 1å¹´ç¼“å­˜
                'Metadata': {
                    'original-id': str(metadata.get('id', '')),
                    'category': str(metadata.get('category', '')),
                    'platform': str(metadata.get('platform', 'pixabay')),
                    'processed': 'true',
                    'background-removed': 'true',
                    'upload-date': datetime.now().isoformat()
                }
            }
            
            # ä¸Šä¼ æ–‡ä»¶
            logger.info(f"ğŸ“¤ Uploading: {filename} -> {s3_key}")
            
            with open(local_path, 'rb') as f:
                self.r2_client.upload_fileobj(
                    f,
                    self.r2_bucket,
                    s3_key,
                    ExtraArgs=upload_metadata
                )
            
            self.stats['uploaded'] += 1
            self.stats['category_stats'][metadata.get('category', 'unknown')] += 1
            
            logger.info(f"âœ… Successfully uploaded: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to upload {local_path}: {e}")
            self.stats['failed'] += 1
            return False
    
    def create_unified_metadata(self, processed_files: List[str]) -> List[Dict[str, Any]]:
        """åˆ›å»ºç»Ÿä¸€çš„å…ƒæ•°æ®æ–‡ä»¶"""
        unified_metadata = []
        
        for png_path in processed_files:
            try:
                # åŠ è½½å…ƒæ•°æ®
                metadata = self.load_image_metadata(png_path)
                
                # æ„å»ºç»Ÿä¸€æ ¼å¼
                filename = os.path.basename(png_path)
                image_id = metadata.get('id', os.path.splitext(filename)[0])
                
                unified_entry = {
                    'id': f"pixabay_{image_id}",
                    'title': metadata.get('title', f"Image {image_id}"),
                    'description': metadata.get('description', metadata.get('title', f"High-quality transparent background image from Pixabay")),
                    'tags': metadata.get('tags', []),
                    'category': metadata.get('category', 'general'),
                    'imageUrl': f"{self.r2_public_url}/images/{filename}",
                    'thumbnailUrl': f"{self.r2_public_url}/images/{filename}",
                    'downloadUrl': f"{self.r2_public_url}/images/{filename}",
                    'width': metadata.get('width', 0),
                    'height': metadata.get('height', 0),
                    'transparencyRatio': 1.0,  # å¤„ç†åçš„å›¾ç‰‡éƒ½æœ‰é€æ˜èƒŒæ™¯
                    'qualityScore': metadata.get('quality_score', 95),
                    'platform': 'pixabay',
                    'author': metadata.get('author', 'Pixabay Contributor'),
                    'authorUrl': metadata.get('author_url', 'https://pixabay.com/'),
                    'uploadDate': datetime.now().isoformat(),
                    'fileSize': os.path.getsize(png_path) if os.path.exists(png_path) else 0,
                    'processingInfo': {
                        'background_removed': True,
                        'file_format': 'PNG',
                        'transparency_added': True,
                        'processed_date': metadata.get('processed_date', datetime.now().isoformat()),
                        'original_query': metadata.get('query_used', ''),
                        'fetch_metadata': metadata.get('fetch_metadata', {})
                    }
                }
                
                unified_metadata.append(unified_entry)
                
            except Exception as e:
                logger.error(f"Error processing metadata for {png_path}: {e}")
        
        return unified_metadata
    
    def run_massive_upload(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¤§æ‰¹é‡ä¸Šä¼ ä»»åŠ¡"""
        logger.info("ğŸš€ Starting massive R2 upload")
        
        # è·å–æ‰€æœ‰PNGæ–‡ä»¶
        png_files = self.get_png_files()
        
        if not png_files:
            logger.warning("âŒ No PNG files found to upload")
            return self.generate_report([])
        
        logger.info(f"ğŸ“Š Uploading {len(png_files)} PNG files to R2...")
        
        # ä¸Šä¼ æ¯ä¸ªæ–‡ä»¶
        uploaded_files = []
        
        for i, png_path in enumerate(png_files, 1):
            logger.info(f"\nğŸ“¤ [{i}/{len(png_files)}] Processing: {os.path.basename(png_path)}")
            
            # åŠ è½½å…ƒæ•°æ®
            metadata = self.load_image_metadata(png_path)
            
            # ä¸Šä¼ æ–‡ä»¶
            if self.upload_single_file(png_path, metadata):
                uploaded_files.append(png_path)
            
            # æ¯50ä¸ªæ–‡ä»¶è¾“å‡ºè¿›åº¦æŠ¥å‘Š
            if i % 50 == 0:
                elapsed = datetime.now() - self.stats['start_time']
                rate = i / elapsed.total_seconds() * 60  # æ¯åˆ†é’Ÿä¸Šä¼ æ•°
                eta_minutes = (len(png_files) - i) / rate if rate > 0 else 0
                
                logger.info(f"""
ğŸ”„ Progress Report - {i}/{len(png_files)} processed
âœ… Uploaded: {self.stats['uploaded']} | âŒ Failed: {self.stats['failed']} | â­ï¸ Skipped: {self.stats['skipped']}
â±ï¸ Speed: {rate:.1f} files/min | ETA: {eta_minutes:.1f} minutes
ğŸ“Š Categories: {dict(self.stats['category_stats'])}
                """)
            
            # çŸ­æš‚ä¼‘æ¯é¿å…è¿‡è½½
            time.sleep(0.1)
        
        return self.generate_report(uploaded_files)
    
    def generate_report(self, uploaded_files: List[str]) -> Dict[str, Any]:\n        \"\"\"ç”Ÿæˆä¸Šä¼ æŠ¥å‘Š\"\"\"\n        end_time = datetime.now()\n        total_duration = end_time - self.stats['start_time']\n        \n        # åˆ›å»ºç»Ÿä¸€å…ƒæ•°æ®\n        logger.info(\"ğŸ“ Creating unified metadata...\")\n        unified_metadata = self.create_unified_metadata(uploaded_files)\n        \n        # ä¿å­˜æ–°çš„å…ƒæ•°æ®åˆ°ç°æœ‰æ–‡ä»¶\n        metadata_file = 'dist/metadata.json'\n        existing_metadata = []\n        \n        # åŠ è½½ç°æœ‰å…ƒæ•°æ®\n        if os.path.exists(metadata_file):\n            try:\n                with open(metadata_file, 'r', encoding='utf-8') as f:\n                    existing_metadata = json.load(f)\n                logger.info(f\"ğŸ“š Loaded {len(existing_metadata)} existing images\")\n            except Exception as e:\n                logger.error(f\"Error loading existing metadata: {e}\")\n                existing_metadata = []\n        \n        # åˆå¹¶å…ƒæ•°æ®ï¼ˆé¿å…é‡å¤ï¼‰\n        existing_ids = {img.get('id') for img in existing_metadata}\n        new_images = [img for img in unified_metadata if img['id'] not in existing_ids]\n        \n        # æ›´æ–°å…ƒæ•°æ®æ–‡ä»¶\n        all_metadata = existing_metadata + new_images\n        \n        with open(metadata_file, 'w', encoding='utf-8') as f:\n            json.dump(all_metadata, f, indent=2, ensure_ascii=False)\n        \n        # ç”ŸæˆæŠ¥å‘Š\n        report = {\n            'summary': {\n                'total_found': self.stats['total_found'],\n                'uploaded': self.stats['uploaded'],\n                'failed': self.stats['failed'],\n                'skipped': self.stats['skipped'],\n                'new_images_added': len(new_images),\n                'total_images_now': len(all_metadata),\n                'success_rate': f\"{self.stats['uploaded'] / max(1, self.stats['total_found']) * 100:.1f}%\",\n                'duration': str(total_duration),\n                'upload_speed': f\"{self.stats['uploaded'] / total_duration.total_seconds() * 60:.1f} files/min\"\n            },\n            'category_distribution': dict(self.stats['category_stats']),\n            'r2_info': {\n                'endpoint': self.r2_endpoint,\n                'bucket': self.r2_bucket,\n                'public_url': self.r2_public_url,\n                'total_files_uploaded': self.stats['uploaded']\n            },\n            'metadata': {\n                'start_time': self.stats['start_time'].isoformat(),\n                'end_time': end_time.isoformat(),\n                'metadata_file': metadata_file,\n                'existing_images': len(existing_metadata),\n                'new_images': len(new_images)\n            }\n        }\n        \n        # ä¿å­˜æŠ¥å‘Š\n        report_file = f\"logs/r2_upload_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n        with open(report_file, 'w') as f:\n            json.dump(report, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"\"\"\nğŸ‰ MASSIVE R2 UPLOAD COMPLETED! ğŸ‰\n\nğŸ“Š Final Statistics:\nâ€¢ Total files found: {self.stats['total_found']}\nâ€¢ Successfully uploaded: {self.stats['uploaded']}\nâ€¢ Failed uploads: {self.stats['failed']}\nâ€¢ Skipped (already exists): {self.stats['skipped']}\nâ€¢ Success rate: {self.stats['uploaded'] / max(1, self.stats['total_found']) * 100:.1f}%\nâ€¢ Total duration: {total_duration}\nâ€¢ Upload speed: {self.stats['uploaded'] / total_duration.total_seconds() * 60:.1f} files/min\n\nğŸ“ Category distribution:\n{chr(10).join(f'â€¢ {cat}: {count} images' for cat, count in self.stats['category_stats'].items())}\n\nğŸŒ R2 Info:\nâ€¢ Bucket: {self.r2_bucket}\nâ€¢ Public URL: {self.r2_public_url}\nâ€¢ Total images now: {len(all_metadata)}\n\nğŸ“„ Files:\nâ€¢ Metadata: {metadata_file}\nâ€¢ Report: {report_file}\n        \"\"\")\n        \n        return report\n\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    try:\n        uploader = MassiveR2Uploader()\n        report = uploader.run_massive_upload()\n        \n        # è¾“å‡ºJSONæ ¼å¼çš„æŠ¥å‘Š\n        print(\"\\n\" + \"=\"*50)\n        print(\"R2 UPLOAD REPORT JSON:\")\n        print(\"=\"*50)\n        print(json.dumps(report, indent=2, ensure_ascii=False))\n        \n    except KeyboardInterrupt:\n        logger.info(\"â¹ï¸ Upload interrupted by user\")\n    except Exception as e:\n        logger.error(f\"ğŸ’¥ Critical error: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()"