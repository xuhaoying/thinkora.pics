#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„å›¾ç‰‡è·å–è„šæœ¬ - ä¸“æ³¨äºè·å–æœ‰æ ‡ç­¾çš„é«˜è´¨é‡å›¾ç‰‡
ä»…ä½¿ç”¨ Unsplash å’Œ Pixabay API
"""

import os
import json
import requests
import logging
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv
import time
import random

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env') or load_dotenv('unsplash/.env')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/fetch_tagged_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# APIé…ç½®
UNSPLASH_ACCESS_KEY = os.getenv('UNSPLASH_ACCESS_KEY')
PIXABAY_API_KEY = os.getenv('PIXABAY_API_KEY')

# ä¼˜åŒ–çš„æœç´¢å…³é”®è¯ - æ›´å…·ä½“å’Œå•†ä¸šåŒ–
SEARCH_QUERIES = [
    # ç§‘æŠ€ç±»
    'laptop computer isolated', 'smartphone mockup', 'headphones product',
    'keyboard mouse setup', 'tablet device', 'smartwatch wearable',
    
    # åŠå…¬ç±»
    'office supplies', 'desk accessories', 'notebook pen', 
    'calculator business', 'clipboard document', 'coffee cup work',
    
    # ç”Ÿæ´»ç±»
    'home decor minimal', 'kitchen utensils', 'bathroom accessories',
    'bedroom furniture', 'living room modern', 'plant indoor',
    
    # ç¾é£Ÿç±»
    'food photography overhead', 'restaurant dish', 'coffee beans',
    'fresh vegetables', 'bakery items', 'healthy snacks',
    
    # æ—¶å°šç±»
    'fashion accessories', 'jewelry product', 'sunglasses eyewear',
    'shoes footwear', 'handbag purse', 'watch timepiece',
    
    # ç¾å®¹å¥åº·
    'skincare products', 'makeup cosmetics', 'perfume bottle',
    'wellness spa', 'fitness equipment', 'yoga mat'
]

class TaggedImageFetcher:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ThinkOraPics/1.0 (https://thinkora.pics)'
        })
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs('raw/unsplash', exist_ok=True)
        os.makedirs('raw/pixabay', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        
        # åŠ è½½å·²ä¸‹è½½è®°å½•
        self.downloaded_ids = self.load_downloaded_ids()
    
    def load_downloaded_ids(self) -> Dict[str, set]:
        """åŠ è½½å·²ä¸‹è½½çš„å›¾ç‰‡ID"""
        downloaded_file = 'downloaded_images.json'
        if os.path.exists(downloaded_file):
            with open(downloaded_file, 'r') as f:
                data = json.load(f)
                return {
                    'unsplash': set(data.get('unsplash', [])),
                    'pixabay': set(data.get('pixabay', []))
                }
        return {'unsplash': set(), 'pixabay': set()}
    
    def save_downloaded_ids(self):
        """ä¿å­˜å·²ä¸‹è½½çš„å›¾ç‰‡ID"""
        data = {
            'unsplash': list(self.downloaded_ids['unsplash']),
            'pixabay': list(self.downloaded_ids['pixabay'])
        }
        with open('downloaded_images.json', 'w') as f:
            json.dump(data, f, indent=2)
    
    def fetch_unsplash_images(self, query: str, per_page: int = 30) -> List[Dict[str, Any]]:
        """ä»Unsplashè·å–å¸¦æ ‡ç­¾çš„å›¾ç‰‡"""
        if not UNSPLASH_ACCESS_KEY:
            logger.warning("Unsplash API key not found")
            return []
        
        try:
            url = 'https://api.unsplash.com/search/photos'
            params = {
                'query': query,
                'per_page': per_page,
                'order_by': 'relevant',  # æŒ‰ç›¸å…³æ€§æ’åº
                'orientation': 'squarish'  # ä¼˜å…ˆè·å–æ–¹å½¢å›¾ç‰‡
            }
            headers = {'Authorization': f'Client-ID {UNSPLASH_ACCESS_KEY}'}
            
            response = self.session.get(url, params=params, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            images = []
            
            for photo in data.get('results', []):
                # åªé€‰æ‹©æœ‰æ ‡ç­¾çš„å›¾ç‰‡
                tags = [tag['title'] for tag in photo.get('tags', [])]
                if len(tags) >= 3 and photo['id'] not in self.downloaded_ids['unsplash']:
                    image_info = {
                        'id': photo['id'],
                        'platform': 'unsplash',
                        'url': photo['urls']['regular'],
                        'download_url': photo['links']['download'],
                        'download_location': photo['links']['download_location'],
                        'width': photo['width'],
                        'height': photo['height'],
                        'color': photo.get('color', '#ffffff'),
                        'description': photo.get('description') or photo.get('alt_description', ''),
                        'tags': tags[:10],  # é™åˆ¶æ ‡ç­¾æ•°é‡
                        'author': photo['user']['name'],
                        'author_url': photo['user']['links']['html'],
                        'likes': photo.get('likes', 0),
                        'quality_score': photo.get('likes', 0) * 10 + len(tags) * 5
                    }
                    images.append(image_info)
                    logger.info(f"Unsplash {photo['id']}: {len(tags)} tags - {', '.join(tags[:5])}")
            
            # æŒ‰è´¨é‡åˆ†æ•°æ’åº
            images.sort(key=lambda x: x['quality_score'], reverse=True)
            return images[:20]  # è¿”å›å‰20ä¸ªæœ€ä½³ç»“æœ
            
        except Exception as e:
            logger.error(f"Error fetching from Unsplash: {e}")
            return []
    
    def fetch_pixabay_images(self, query: str, per_page: int = 50) -> List[Dict[str, Any]]:
        """ä»Pixabayè·å–å¸¦æ ‡ç­¾çš„å›¾ç‰‡"""
        if not PIXABAY_API_KEY:
            logger.warning("Pixabay API key not found")
            return []
        
        try:
            url = 'https://pixabay.com/api/'
            params = {
                'key': PIXABAY_API_KEY,
                'q': query,
                'per_page': per_page,
                'image_type': 'photo',
                'min_width': 1000,
                'min_height': 800,
                'safesearch': 'true',
                'order': 'popular'
            }
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            images = []
            
            for photo in data.get('hits', []):
                # è§£ææ ‡ç­¾
                tags = []
                if photo.get('tags'):
                    tags = [tag.strip() for tag in photo['tags'].split(',')]
                
                # åªé€‰æ‹©æœ‰è¶³å¤Ÿæ ‡ç­¾çš„å›¾ç‰‡
                if len(tags) >= 3 and str(photo['id']) not in self.downloaded_ids['pixabay']:
                    image_info = {
                        'id': str(photo['id']),
                        'platform': 'pixabay',
                        'url': photo['largeImageURL'],
                        'download_url': photo['largeImageURL'],
                        'width': photo['imageWidth'],
                        'height': photo['imageHeight'],
                        'description': f"{', '.join(tags[:3])} - Professional stock photo",
                        'tags': tags[:10],  # é™åˆ¶æ ‡ç­¾æ•°é‡
                        'author': photo['user'],
                        'author_url': f"https://pixabay.com/users/{photo['user']}-{photo['user_id']}/",
                        'views': photo.get('views', 0),
                        'downloads': photo.get('downloads', 0),
                        'likes': photo.get('likes', 0),
                        'quality_score': photo.get('likes', 0) + photo.get('downloads', 0) + len(tags) * 10
                    }
                    images.append(image_info)
                    logger.info(f"Pixabay {photo['id']}: {len(tags)} tags - {', '.join(tags[:5])}")
            
            # æŒ‰è´¨é‡åˆ†æ•°æ’åº
            images.sort(key=lambda x: x['quality_score'], reverse=True)
            return images[:30]  # è¿”å›å‰30ä¸ªæœ€ä½³ç»“æœ
            
        except Exception as e:
            logger.error(f"Error fetching from Pixabay: {e}")
            return []
    
    def download_image(self, image_info: Dict[str, Any]) -> bool:
        """ä¸‹è½½å•å¼ å›¾ç‰‡åŠå…¶å…ƒæ•°æ®"""
        try:
            platform = image_info['platform']
            image_id = image_info['id']
            url = image_info['download_url']
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            filename = f"{platform}_{image_id}.jpg"
            filepath = os.path.join('raw', platform, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
            if os.path.exists(filepath):
                logger.info(f"Image already exists: {filename}")
                return True
            
            # ç‰¹æ®Šå¤„ç† Unsplash ä¸‹è½½ï¼ˆéœ€è¦è§¦å‘ä¸‹è½½äº‹ä»¶ï¼‰
            if platform == 'unsplash' and 'download_location' in image_info:
                try:
                    # è§¦å‘ä¸‹è½½äº‹ä»¶
                    headers = {'Authorization': f'Client-ID {UNSPLASH_ACCESS_KEY}'}
                    self.session.get(image_info['download_location'], headers=headers)
                except:
                    pass  # å¿½ç•¥é”™è¯¯ï¼Œç»§ç»­ä¸‹è½½
            
            # ä¸‹è½½å›¾ç‰‡
            response = self.session.get(url, stream=True)
            response.raise_for_status()
            
            # ä¿å­˜å›¾ç‰‡
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # ä¿å­˜å…ƒæ•°æ®ï¼ˆåŒ…å«æ ‡ç­¾ï¼‰
            metadata_path = filepath.replace('.jpg', '_metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(image_info, f, indent=2, ensure_ascii=False)
            
            # è®°å½•å·²ä¸‹è½½
            self.downloaded_ids[platform].add(image_id)
            logger.info(f"Downloaded: {filename} with {len(image_info.get('tags', []))} tags")
            
            # å»¶è¿Ÿä»¥é¿å…é¢‘ç‡é™åˆ¶
            time.sleep(random.uniform(1, 2))
            return True
            
        except Exception as e:
            logger.error(f"Error downloading image {image_info['id']}: {e}")
            return False
    
    def fetch_quality_images(self, total_images: int = 50):
        """è·å–é«˜è´¨é‡å¸¦æ ‡ç­¾çš„å›¾ç‰‡"""
        logger.info(f"Starting to fetch {total_images} quality images with tags")
        
        all_images = []
        images_per_query = max(2, total_images // len(SEARCH_QUERIES))
        
        # éšæœºé€‰æ‹©æŸ¥è¯¢è¯ï¼Œé¿å…é‡å¤
        selected_queries = random.sample(SEARCH_QUERIES, min(len(SEARCH_QUERIES), total_images // 2))
        
        for query in selected_queries:
            logger.info(f"\nğŸ” Searching for: {query}")
            
            # ä» Unsplash è·å–
            unsplash_images = self.fetch_unsplash_images(query, 30)
            all_images.extend(unsplash_images)
            logger.info(f"  Unsplash: Found {len(unsplash_images)} images with tags")
            
            # ä» Pixabay è·å–
            pixabay_images = self.fetch_pixabay_images(query, 50)
            all_images.extend(pixabay_images)
            logger.info(f"  Pixabay: Found {len(pixabay_images)} images with tags")
            
            # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿçš„å›¾ç‰‡ï¼Œåœæ­¢
            if len(all_images) >= total_images * 2:
                break
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        # å»é‡ï¼ˆåŸºäºIDï¼‰
        unique_images = {}
        for img in all_images:
            key = f"{img['platform']}_{img['id']}"
            if key not in unique_images:
                unique_images[key] = img
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€å¥½çš„
        sorted_images = sorted(unique_images.values(), key=lambda x: x['quality_score'], reverse=True)
        selected_images = sorted_images[:total_images]
        
        logger.info(f"\nğŸ“Š Summary:")
        logger.info(f"  Total unique images found: {len(unique_images)}")
        logger.info(f"  Selected top {len(selected_images)} images")
        
        # ä¸‹è½½é€‰ä¸­çš„å›¾ç‰‡
        downloaded_count = 0
        for i, image in enumerate(selected_images, 1):
            logger.info(f"\n[{i}/{len(selected_images)}] Downloading {image['platform']}_{image['id']}")
            logger.info(f"  Tags: {', '.join(image['tags'][:5])}...")
            
            if self.download_image(image):
                downloaded_count += 1
        
        # ä¿å­˜å·²ä¸‹è½½è®°å½•
        self.save_downloaded_ids()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_found': len(unique_images),
            'selected': len(selected_images),
            'downloaded': downloaded_count,
            'platforms': {
                'unsplash': len([img for img in selected_images if img['platform'] == 'unsplash']),
                'pixabay': len([img for img in selected_images if img['platform'] == 'pixabay'])
            },
            'average_tags': sum(len(img['tags']) for img in selected_images) / len(selected_images) if selected_images else 0,
            'queries_used': selected_queries
        }
        
        with open(f'logs/fetch_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"\nâœ… Fetch completed! Downloaded {downloaded_count} images with tags")
        logger.info(f"ğŸ“Š Average tags per image: {report['average_tags']:.1f}")
        
        return report


if __name__ == '__main__':
    fetcher = TaggedImageFetcher()
    
    # è·å–50å¼ é«˜è´¨é‡å¸¦æ ‡ç­¾çš„å›¾ç‰‡
    fetcher.fetch_quality_images(50)