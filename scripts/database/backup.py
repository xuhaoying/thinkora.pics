#!/usr/bin/env python3
"""
æ•°æ®åº“å¤‡ä»½å’Œç®¡ç†è„šæœ¬
"""

import os
import shutil
import sqlite3
import argparse
from datetime import datetime
from pathlib import Path
import json

class DatabaseManager:
    def __init__(self):
        self.db_path = Path("images.db")
        self.backup_dir = Path("backups")
        self.backup_dir.mkdir(exist_ok=True)
    
    def create_backup(self):
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        if not self.db_path.exists():
            print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = self.backup_dir / f"images_backup_{timestamp}.db"
        
        try:
            shutil.copy2(self.db_path, backup_path)
            print(f"âœ… æ•°æ®åº“å¤‡ä»½æˆåŠŸ: {backup_path}")
            
            # åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
            info_path = backup_path.with_suffix('.json')
            backup_info = {
                "timestamp": timestamp,
                "original_file": str(self.db_path),
                "backup_file": str(backup_path),
                "file_size": backup_path.stat().st_size,
                "created_at": datetime.now().isoformat()
            }
            
            with open(info_path, 'w') as f:
                json.dump(backup_info, f, indent=2)
            
            return True
        except Exception as e:
            print(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def list_backups(self):
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½æ–‡ä»¶"""
        backups = list(self.backup_dir.glob("images_backup_*.db"))
        backups.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        if not backups:
            print("ğŸ“‹ æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")
            return []
        
        print("ğŸ“‹ å¤‡ä»½æ–‡ä»¶åˆ—è¡¨:")
        for backup in backups:
            stat = backup.stat()
            size_mb = stat.st_size / (1024 * 1024)
            mtime = datetime.fromtimestamp(stat.st_mtime)
            print(f"  {backup.name} ({size_mb:.1f}MB, {mtime.strftime('%Y-%m-%d %H:%M:%S')})")
        
        return backups
    
    def restore_backup(self, backup_file):
        """ä»å¤‡ä»½æ¢å¤æ•°æ®åº“"""
        backup_path = Path(backup_file)
        
        if not backup_path.exists():
            print(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")
            return False
        
        try:
            # å¤‡ä»½å½“å‰æ•°æ®åº“
            if self.db_path.exists():
                current_backup = f"images_before_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
                shutil.copy2(self.db_path, self.backup_dir / current_backup)
                print(f"ğŸ“¦ å½“å‰æ•°æ®åº“å·²å¤‡ä»½ä¸º: {current_backup}")
            
            # æ¢å¤æ•°æ®åº“
            shutil.copy2(backup_path, self.db_path)
            print(f"âœ… æ•°æ®åº“æ¢å¤æˆåŠŸ: {backup_path} -> {self.db_path}")
            return True
            
        except Exception as e:
            print(f"âŒ æ¢å¤å¤±è´¥: {e}")
            return False
    
    def cleanup_old_backups(self, keep_count=10):
        """æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶"""
        backups = list(self.backup_dir.glob("images_backup_*.db"))
        backups.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        if len(backups) <= keep_count:
            print(f"ğŸ“‹ å½“å‰æœ‰ {len(backups)} ä¸ªå¤‡ä»½ï¼Œæ— éœ€æ¸…ç†")
            return 0
        
        to_delete = backups[keep_count:]
        deleted_count = 0
        
        for backup in to_delete:
            try:
                # åˆ é™¤å¤‡ä»½æ–‡ä»¶å’Œå¯¹åº”çš„infoæ–‡ä»¶
                backup.unlink()
                info_file = backup.with_suffix('.json')
                if info_file.exists():
                    info_file.unlink()
                deleted_count += 1
                print(f"ğŸ—‘ï¸ åˆ é™¤æ—§å¤‡ä»½: {backup.name}")
            except Exception as e:
                print(f"âŒ åˆ é™¤å¤±è´¥ {backup.name}: {e}")
        
        print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§å¤‡ä»½")
        return deleted_count
    
    def get_database_info(self):
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        if not self.db_path.exists():
            return {"error": "æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨"}
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # è·å–è¡¨ä¿¡æ¯
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            
            info = {
                "file_path": str(self.db_path),
                "file_size": self.db_path.stat().st_size,
                "file_size_mb": self.db_path.stat().st_size / (1024 * 1024),
                "modified_time": datetime.fromtimestamp(self.db_path.stat().st_mtime).isoformat(),
                "tables": tables
            }
            
            # è·å–imagesè¡¨çš„è¯¦ç»†ä¿¡æ¯
            if 'images' in tables:
                cursor.execute("SELECT COUNT(*) FROM images")
                total_images = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(*) FROM images WHERE processed = TRUE")
                processed_images = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(*) FROM images WHERE uploaded = TRUE")
                uploaded_images = cursor.fetchone()[0]
                
                cursor.execute("SELECT source, COUNT(*) FROM images GROUP BY source")
                source_stats = dict(cursor.fetchall())
                
                info["images"] = {
                    "total": total_images,
                    "processed": processed_images,
                    "uploaded": uploaded_images,
                    "pending": total_images - processed_images,
                    "by_source": source_stats
                }
            
            conn.close()
            return info
            
        except Exception as e:
            return {"error": f"è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}"}
    
    def optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“"""
        if not self.db_path.exists():
            print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        try:
            conn = sqlite3.connect(self.db_path)
            
            print("ğŸ”„ å¼€å§‹ä¼˜åŒ–æ•°æ®åº“...")
            
            # åˆ›å»ºç´¢å¼•
            indexes = [
                "CREATE INDEX IF NOT EXISTS idx_images_processed ON images(processed)",
                "CREATE INDEX IF NOT EXISTS idx_images_uploaded ON images(uploaded)",
                "CREATE INDEX IF NOT EXISTS idx_images_source ON images(source)",
                "CREATE INDEX IF NOT EXISTS idx_images_created_at ON images(created_at)"
            ]
            
            for index_sql in indexes:
                conn.execute(index_sql)
            
            # æ‰§è¡ŒVACUUMæ¥é‡æ–°ç»„ç»‡æ•°æ®åº“
            conn.execute("VACUUM")
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            conn.execute("ANALYZE")
            
            conn.commit()
            conn.close()
            
            print("âœ… æ•°æ®åº“ä¼˜åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–å¤±è´¥: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="æ•°æ®åº“ç®¡ç†å·¥å…·")
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # å¤‡ä»½å‘½ä»¤
    subparsers.add_parser("backup", help="åˆ›å»ºæ•°æ®åº“å¤‡ä»½")
    
    # åˆ—å‡ºå¤‡ä»½
    subparsers.add_parser("list", help="åˆ—å‡ºæ‰€æœ‰å¤‡ä»½")
    
    # æ¢å¤å¤‡ä»½
    restore_parser = subparsers.add_parser("restore", help="ä»å¤‡ä»½æ¢å¤æ•°æ®åº“")
    restore_parser.add_argument("backup_file", help="å¤‡ä»½æ–‡ä»¶è·¯å¾„")
    
    # æ¸…ç†å¤‡ä»½
    cleanup_parser = subparsers.add_parser("cleanup", help="æ¸…ç†æ—§å¤‡ä»½")
    cleanup_parser.add_argument("--keep", type=int, default=10, help="ä¿ç•™å¤‡ä»½æ•°é‡")
    
    # æ•°æ®åº“ä¿¡æ¯
    subparsers.add_parser("info", help="æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯")
    
    # ä¼˜åŒ–æ•°æ®åº“
    subparsers.add_parser("optimize", help="ä¼˜åŒ–æ•°æ®åº“")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = DatabaseManager()
    
    if args.command == "backup":
        manager.create_backup()
    elif args.command == "list":
        manager.list_backups()
    elif args.command == "restore":
        manager.restore_backup(args.backup_file)
    elif args.command == "cleanup":
        manager.cleanup_old_backups(args.keep)
    elif args.command == "info":
        info = manager.get_database_info()
        if "error" in info:
            print(f"âŒ {info['error']}")
        else:
            print("ğŸ“Š æ•°æ®åº“ä¿¡æ¯:")
            print(f"  æ–‡ä»¶è·¯å¾„: {info['file_path']}")
            print(f"  æ–‡ä»¶å¤§å°: {info['file_size_mb']:.1f}MB")
            print(f"  ä¿®æ”¹æ—¶é—´: {info['modified_time']}")
            print(f"  è¡¨æ•°é‡: {len(info['tables'])}")
            
            if "images" in info:
                img_info = info["images"]
                print(f"\nğŸ“¸ å›¾ç‰‡ç»Ÿè®¡:")
                print(f"  æ€»æ•°: {img_info['total']}")
                print(f"  å·²å¤„ç†: {img_info['processed']}")
                print(f"  å·²ä¸Šä¼ : {img_info['uploaded']}")
                print(f"  å¾…å¤„ç†: {img_info['pending']}")
                print(f"  æ¥æºåˆ†å¸ƒ: {img_info['by_source']}")
    elif args.command == "optimize":
        manager.optimize_database()

if __name__ == "__main__":
    main()