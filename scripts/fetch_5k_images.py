#!/usr/bin/env python3
"""
è·å–5000å¼ é«˜è´¨é‡å¸¦æ ‡ç­¾çš„å›¾ç‰‡
æ”¯æŒUnsplashå’ŒPixabayï¼Œåˆ†æ‰¹è·å–ï¼Œé¿å…APIé™åˆ¶
"""

import os
import json
import requests
import time
import random
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env') or load_dotenv('unsplash/.env')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/fetch_5k_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# APIé…ç½®
UNSPLASH_ACCESS_KEY = os.getenv('UNSPLASH_ACCESS_KEY')
PIXABAY_API_KEY = os.getenv('PIXABAY_API_KEY')

# æ‰©å±•çš„æœç´¢å…³é”®è¯åº“
SEARCH_KEYWORDS = {
    'technology': [
        'laptop computer', 'smartphone device', 'tablet screen', 'headphones audio',
        'keyboard typing', 'mouse click', 'monitor display', 'camera photography',
        'smartwatch wearable', 'drone flying', 'virtual reality', 'gaming console',
        'usb cable', 'wireless charger', 'bluetooth speaker', 'microphone podcast'
    ],
    'business': [
        'office desk', 'meeting room', 'business card', 'handshake deal',
        'presentation slides', 'financial chart', 'calculator accounting', 'contract document',
        'team collaboration', 'corporate building', 'startup workspace', 'entrepreneur laptop',
        'invoice payment', 'business strategy', 'marketing plan', 'sales report'
    ],
    'lifestyle': [
        'coffee morning', 'healthy breakfast', 'yoga mat', 'fitness workout',
        'reading book', 'travel suitcase', 'home interior', 'cozy bedroom',
        'kitchen cooking', 'fashion style', 'beauty skincare', 'wellness spa',
        'outdoor adventure', 'hobby craft', 'pet companion', 'family time'
    ],
    'nature': [
        'plant indoor', 'flower bouquet', 'succulent pot', 'tree forest',
        'mountain landscape', 'ocean beach', 'sunset sky', 'garden outdoor',
        'leaf autumn', 'rain weather', 'snow winter', 'spring bloom',
        'cactus desert', 'bamboo zen', 'herb garden', 'botanical tropical'
    ],
    'food': [
        'fresh fruit', 'vegetable organic', 'coffee cup', 'tea ceremony',
        'bakery bread', 'dessert sweet', 'salad healthy', 'pasta italian',
        'sushi japanese', 'pizza slice', 'smoothie drink', 'chocolate treat',
        'wine glass', 'cocktail bar', 'breakfast meal', 'dinner plate'
    ],
    'education': [
        'book study', 'notebook writing', 'pencil drawing', 'classroom teaching',
        'student learning', 'graduation cap', 'library books', 'online course',
        'homework assignment', 'science lab', 'math equation', 'art supplies',
        'music instrument', 'language learning', 'exam test', 'school supplies'
    ],
    'health': [
        'medical stethoscope', 'pills medicine', 'doctor consultation', 'hospital care',
        'fitness equipment', 'yoga pose', 'meditation calm', 'nutrition food',
        'mental health', 'first aid', 'thermometer fever', 'mask protection',
        'vaccine injection', 'dental care', 'eye glasses', 'wellness therapy'
    ],
    'design': [
        'color palette', 'sketch drawing', 'graphic design', 'typography font',
        'logo brand', 'mockup template', 'wireframe ui', 'prototype ux',
        'illustration art', 'pattern texture', 'icon set', 'poster creative',
        'business card', 'brochure layout', 'web design', 'mobile app'
    ]
}

class MassiveImageFetcher:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ThinkOraPics/2.0 (https://thinkora.pics)'
        })
        
        # åˆ›å»ºç›®å½•
        os.makedirs('raw/unsplash', exist_ok=True)
        os.makedirs('raw/pixabay', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        
        # åŠ è½½å·²ä¸‹è½½è®°å½•
        self.downloaded_ids = self.load_downloaded_ids()
        self.stats = {
            'unsplash': {'fetched': 0, 'downloaded': 0},
            'pixabay': {'fetched': 0, 'downloaded': 0},
            'total_with_tags': 0
        }
    
    def load_downloaded_ids(self) -> Dict[str, set]:
        """åŠ è½½å·²ä¸‹è½½çš„å›¾ç‰‡ID"""
        downloaded_file = 'downloaded_images.json'
        if os.path.exists(downloaded_file):
            with open(downloaded_file, 'r') as f:
                data = json.load(f)
                return {
                    'unsplash': set(data.get('unsplash', [])),
                    'pixabay': set(data.get('pixabay', []))
                }
        return {'unsplash': set(), 'pixabay': set()}
    
    def save_downloaded_ids(self):
        """ä¿å­˜å·²ä¸‹è½½çš„å›¾ç‰‡ID"""
        data = {
            'unsplash': list(self.downloaded_ids['unsplash']),
            'pixabay': list(self.downloaded_ids['pixabay'])
        }
        with open('downloaded_images.json', 'w') as f:
            json.dump(data, f, indent=2)
    
    def fetch_unsplash_batch(self, query: str, page: int = 1, per_page: int = 30) -> List[Dict]:
        """ä»Unsplashè·å–ä¸€æ‰¹å›¾ç‰‡"""
        if not UNSPLASH_ACCESS_KEY:
            return []
        
        try:
            url = 'https://api.unsplash.com/search/photos'
            params = {
                'query': query,
                'page': page,
                'per_page': per_page,
                'order_by': 'relevant'
            }
            headers = {'Authorization': f'Client-ID {UNSPLASH_ACCESS_KEY}'}
            
            response = self.session.get(url, params=params, headers=headers)
            
            # æ£€æŸ¥rate limit
            remaining = int(response.headers.get('X-Ratelimit-Remaining', 0))
            if remaining < 10:
                logger.warning(f"Unsplash rate limit low: {remaining}")
                time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
            
            response.raise_for_status()
            data = response.json()
            
            images = []
            for photo in data.get('results', []):
                tags = [tag['title'] for tag in photo.get('tags', [])]
                if len(tags) >= 3 and photo['id'] not in self.downloaded_ids['unsplash']:
                    image_info = {
                        'id': photo['id'],
                        'platform': 'unsplash',
                        'url': photo['urls']['regular'],
                        'download_url': photo['links']['download'],
                        'download_location': photo['links']['download_location'],
                        'width': photo['width'],
                        'height': photo['height'],
                        'description': photo.get('description') or photo.get('alt_description', ''),
                        'tags': tags[:15],  # é™åˆ¶æ ‡ç­¾æ•°é‡
                        'author': photo['user']['name'],
                        'author_url': photo['user']['links']['html'],
                        'likes': photo.get('likes', 0)
                    }
                    images.append(image_info)
                    self.stats['unsplash']['fetched'] += 1
            
            return images
            
        except Exception as e:
            logger.error(f"Unsplash error: {e}")
            return []
    
    def fetch_pixabay_batch(self, query: str, page: int = 1, per_page: int = 100) -> List[Dict]:
        """ä»Pixabayè·å–ä¸€æ‰¹å›¾ç‰‡"""
        if not PIXABAY_API_KEY:
            return []
        
        try:
            url = 'https://pixabay.com/api/'
            params = {
                'key': PIXABAY_API_KEY,
                'q': query,
                'page': page,
                'per_page': min(per_page, 200),  # Pixabayæœ€å¤§200
                'image_type': 'photo',
                'min_width': 800,
                'min_height': 600,
                'safesearch': 'true',
                'order': 'popular'
            }
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            images = []
            
            for photo in data.get('hits', []):
                tags = []
                if photo.get('tags'):
                    tags = [tag.strip() for tag in photo['tags'].split(',')]
                
                if len(tags) >= 3 and str(photo['id']) not in self.downloaded_ids['pixabay']:
                    image_info = {
                        'id': str(photo['id']),
                        'platform': 'pixabay',
                        'url': photo['largeImageURL'],
                        'download_url': photo['largeImageURL'],
                        'width': photo['imageWidth'],
                        'height': photo['imageHeight'],
                        'description': f"{', '.join(tags[:3])} stock photo",
                        'tags': tags[:15],
                        'author': photo['user'],
                        'author_url': f"https://pixabay.com/users/{photo['user']}-{photo['user_id']}/",
                        'views': photo.get('views', 0),
                        'downloads': photo.get('downloads', 0),
                        'likes': photo.get('likes', 0)
                    }
                    images.append(image_info)
                    self.stats['pixabay']['fetched'] += 1
            
            return images
            
        except Exception as e:
            logger.error(f"Pixabay error: {e}")
            return []
    
    def download_image(self, image_info: Dict) -> bool:
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            platform = image_info['platform']
            image_id = image_info['id']
            url = image_info['download_url']
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            filename = f"{platform}_{image_id}.jpg"
            filepath = os.path.join('raw', platform, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
            if os.path.exists(filepath):
                return True
            
            # Unsplashéœ€è¦è§¦å‘ä¸‹è½½äº‹ä»¶
            if platform == 'unsplash' and 'download_location' in image_info:
                try:
                    headers = {'Authorization': f'Client-ID {UNSPLASH_ACCESS_KEY}'}
                    self.session.get(image_info['download_location'], headers=headers)
                except:
                    pass
            
            # ä¸‹è½½å›¾ç‰‡
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # ä¿å­˜å›¾ç‰‡
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_path = filepath.replace('.jpg', '_metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(image_info, f, indent=2, ensure_ascii=False)
            
            # è®°å½•å·²ä¸‹è½½
            self.downloaded_ids[platform].add(image_id)
            self.stats[platform]['downloaded'] += 1
            self.stats['total_with_tags'] += 1
            
            return True
            
        except Exception as e:
            logger.error(f"Download error {image_info['id']}: {e}")
            return False
    
    def fetch_5k_images(self, target_count: int = 5000):
        """è·å–5000å¼ å›¾ç‰‡çš„ä¸»å‡½æ•°"""
        logger.info(f"ğŸš€ å¼€å§‹è·å– {target_count} å¼ å¸¦æ ‡ç­¾çš„å›¾ç‰‡")
        logger.info(f"ğŸ“Š å·²æœ‰å›¾ç‰‡: Unsplash {len(self.downloaded_ids['unsplash'])}, Pixabay {len(self.downloaded_ids['pixabay'])}")
        
        all_images = []
        
        # ç”Ÿæˆæ‰€æœ‰æœç´¢ç»„åˆ
        search_queries = []
        for category, keywords in SEARCH_KEYWORDS.items():
            for keyword in keywords:
                search_queries.append(f"{keyword} {category}")
        
        # éšæœºæ‰“ä¹±æœç´¢é¡ºåº
        random.shuffle(search_queries)
        
        # åˆ†æ‰¹è·å–å›¾ç‰‡
        batch_size = 100
        current_count = len(self.downloaded_ids['unsplash']) + len(self.downloaded_ids['pixabay'])
        
        for query_idx, query in enumerate(search_queries):
            if current_count >= target_count:
                break
            
            logger.info(f"\nğŸ” [{query_idx+1}/{len(search_queries)}] æœç´¢: {query}")
            
            # ä»Pixabayè·å–ï¼ˆæ›´å¤šå…è´¹é¢åº¦ï¼‰
            for page in range(1, 6):  # è·å–å‰5é¡µ
                pixabay_images = self.fetch_pixabay_batch(query, page, 200)
                if not pixabay_images:
                    break
                all_images.extend(pixabay_images)
                logger.info(f"  Pixabayç¬¬{page}é¡µ: è·å¾— {len(pixabay_images)} å¼ å›¾ç‰‡")
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            # ä»Unsplashè·å–ï¼ˆè´¨é‡æ›´é«˜ä½†é¢åº¦æœ‰é™ï¼‰
            for page in range(1, 4):  # è·å–å‰3é¡µ
                unsplash_images = self.fetch_unsplash_batch(query, page, 30)
                if not unsplash_images:
                    break
                all_images.extend(unsplash_images)
                logger.info(f"  Unsplashç¬¬{page}é¡µ: è·å¾— {len(unsplash_images)} å¼ å›¾ç‰‡")
                time.sleep(2)  # Unsplashé™åˆ¶æ›´ä¸¥æ ¼
            
            # å®šæœŸä¿å­˜è¿›åº¦
            if len(all_images) >= batch_size:
                logger.info(f"\nğŸ“¥ ä¸‹è½½æ‰¹æ¬¡ {len(all_images)} å¼ å›¾ç‰‡...")
                self.download_batch(all_images[:batch_size])
                all_images = all_images[batch_size:]
                current_count = self.stats['total_with_tags']
                logger.info(f"âœ… å½“å‰è¿›åº¦: {current_count}/{target_count}")
                
                # ä¿å­˜è¿›åº¦
                self.save_downloaded_ids()
                self.save_progress_report()
        
        # ä¸‹è½½å‰©ä½™å›¾ç‰‡
        if all_images:
            logger.info(f"\nğŸ“¥ ä¸‹è½½æœ€å {len(all_images)} å¼ å›¾ç‰‡...")
            self.download_batch(all_images)
        
        # æœ€ç»ˆæŠ¥å‘Š
        self.save_final_report(target_count)
        
    def download_batch(self, images: List[Dict]):
        """æ‰¹é‡ä¸‹è½½å›¾ç‰‡"""
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {executor.submit(self.download_image, img): img for img in images}
            
            for future in as_completed(futures):
                result = future.result()
                if result:
                    img = futures[future]
                    logger.debug(f"âœ… ä¸‹è½½æˆåŠŸ: {img['platform']}_{img['id']}")
    
    def save_progress_report(self):
        """ä¿å­˜è¿›åº¦æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats,
            'total_downloaded': len(self.downloaded_ids['unsplash']) + len(self.downloaded_ids['pixabay'])
        }
        
        with open('logs/fetch_5k_progress.json', 'w') as f:
            json.dump(report, f, indent=2)
    
    def save_final_report(self, target_count: int):
        """ä¿å­˜æœ€ç»ˆæŠ¥å‘Š"""
        total = self.stats['total_with_tags']
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'target': target_count,
            'achieved': total,
            'completion_rate': f"{(total/target_count*100):.1f}%",
            'platforms': {
                'unsplash': {
                    'fetched': self.stats['unsplash']['fetched'],
                    'downloaded': self.stats['unsplash']['downloaded'],
                    'total': len(self.downloaded_ids['unsplash'])
                },
                'pixabay': {
                    'fetched': self.stats['pixabay']['fetched'],
                    'downloaded': self.stats['pixabay']['downloaded'],
                    'total': len(self.downloaded_ids['pixabay'])
                }
            }
        }
        
        report_path = f'logs/fetch_5k_final_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š æœ€ç»ˆæŠ¥å‘Š:")
        logger.info(f"  ç›®æ ‡: {target_count} å¼ ")
        logger.info(f"  å®Œæˆ: {total} å¼  ({report['completion_rate']})")
        logger.info(f"  Unsplash: {report['platforms']['unsplash']['total']} å¼ ")
        logger.info(f"  Pixabay: {report['platforms']['pixabay']['total']} å¼ ")
        logger.info(f"  æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        logger.info(f"{'='*60}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è·å–5000å¼ å¸¦æ ‡ç­¾çš„å›¾ç‰‡')
    parser.add_argument('--count', type=int, default=5000, help='ç›®æ ‡å›¾ç‰‡æ•°é‡')
    parser.add_argument('--resume', action='store_true', help='ç»§ç»­ä¸Šæ¬¡çš„ä¸‹è½½')
    args = parser.parse_args()
    
    fetcher = MassiveImageFetcher()
    
    if args.resume:
        current = len(fetcher.downloaded_ids['unsplash']) + len(fetcher.downloaded_ids['pixabay'])
        logger.info(f"ğŸ“‚ ç»§ç»­ä¸‹è½½ï¼Œå½“å‰å·²æœ‰ {current} å¼ å›¾ç‰‡")
    
    fetcher.fetch_5k_images(args.count)

if __name__ == '__main__':
    main()