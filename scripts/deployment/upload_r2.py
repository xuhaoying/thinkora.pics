#!/usr/bin/env python3
"""
R2å­˜å‚¨ä¸Šä¼ è„šæœ¬ - å°†å¤„ç†åçš„PNGå›¾ç‰‡ä¸Šä¼ åˆ°Cloudflare R2
"""

import os
import sys
import sqlite3
import boto3
import argparse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from botocore.config import Config
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class R2Uploader:
    def __init__(self):
        self.db_path = "images.db"
        self.processed_dir = Path("processed_images")
        
        # R2é…ç½®
        self.access_key = os.getenv('R2_ACCESS_KEY_ID')
        self.secret_key = os.getenv('R2_SECRET_ACCESS_KEY')
        self.account_id = os.getenv('R2_ACCOUNT_ID')
        self.bucket_name = os.getenv('R2_BUCKET_NAME', 'thinkora-pics')
        self.public_url = os.getenv('R2_PUBLIC_URL', 'https://img.thinkora.pics')
        
        if not all([self.access_key, self.secret_key, self.account_id]):
            print("âŒ è¯·é…ç½®R2ç¯å¢ƒå˜é‡ (R2_ACCESS_KEY_ID, R2_SECRET_ACCESS_KEY, R2_ACCOUNT_ID)")
            sys.exit(1)
        
        # åˆå§‹åŒ–S3å®¢æˆ·ç«¯
        self.endpoint_url = f"https://{self.account_id}.r2.cloudflarestorage.com"
        self.s3_client = boto3.client(
            's3',
            endpoint_url=self.endpoint_url,
            aws_access_key_id=self.access_key,
            aws_secret_access_key=self.secret_key,
            config=Config(
                region_name='auto',
                retries={'max_attempts': 3}
            )
        )
    
    def test_connection(self):
        """æµ‹è¯•R2è¿æ¥"""
        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                MaxKeys=1
            )
            print("âœ… R2è¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ R2è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def get_pending_uploads(self):
        """è·å–å¾…ä¸Šä¼ çš„å›¾ç‰‡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT * FROM images 
            WHERE processed = TRUE AND uploaded = FALSE
        """)
        
        images = cursor.fetchall()
        columns = [description[0] for description in cursor.description]
        conn.close()
        
        return [dict(zip(columns, row)) for row in images]
    
    def get_uploaded_files(self):
        """è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨"""
        try:
            uploaded = set()
            paginator = self.s3_client.get_paginator('list_objects_v2')
            
            for page in paginator.paginate(Bucket=self.bucket_name, Prefix='images/'):
                if 'Contents' in page:
                    for obj in page['Contents']:
                        uploaded.add(obj['Key'])
            
            return uploaded
        except Exception as e:
            print(f"âŒ è·å–å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return set()
    
    def upload_single_file(self, image_data, force=False):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
        image_id = image_data['id']
        local_path = self.processed_dir / f"{image_id}.png"
        r2_key = f"images/{image_id}.png"
        
        if not local_path.exists():
            return False, f"æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_path}"
        
        # æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ 
        if not force:
            try:
                self.s3_client.head_object(Bucket=self.bucket_name, Key=r2_key)
                return True, "æ–‡ä»¶å·²å­˜åœ¨"
            except:
                pass  # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»§ç»­ä¸Šä¼ 
        
        try:
            # ä¸Šä¼ æ–‡ä»¶
            extra_args = {
                'ContentType': 'image/png',
                'CacheControl': 'public, max-age=31536000'  # 1å¹´ç¼“å­˜
            }
            
            self.s3_client.upload_file(
                str(local_path),
                self.bucket_name,
                r2_key,
                ExtraArgs=extra_args
            )
            
            # æ›´æ–°æ•°æ®åº“
            self.mark_as_uploaded(image_id, f"{self.public_url}/{r2_key}")
            
            return True, "ä¸Šä¼ æˆåŠŸ"
            
        except Exception as e:
            return False, f"ä¸Šä¼ å¤±è´¥: {e}"
    
    def mark_as_uploaded(self, image_id, public_url):
        """æ ‡è®°ä¸ºå·²ä¸Šä¼ """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE images 
            SET uploaded = TRUE,
                uploaded_at = ?,
                url_regular = ?,
                url_download = ?
            WHERE id = ?
        """, (datetime.now().isoformat(), public_url, public_url, image_id))
        
        conn.commit()
        conn.close()
    
    def upload_batch(self, force=False, max_workers=5):
        """æ‰¹é‡ä¸Šä¼ """
        if not self.test_connection():
            return 0
        
        images = self.get_pending_uploads()
        
        if not images:
            print("ğŸ“‹ æ²¡æœ‰å¾…ä¸Šä¼ çš„å›¾ç‰‡")
            return 0
        
        print(f"ğŸš€ å¼€å§‹ä¸Šä¼  {len(images)} å¼ å›¾ç‰‡åˆ°R2...")
        
        success_count = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_image = {
                executor.submit(self.upload_single_file, image, force): image 
                for image in images
            }
            
            # å¤„ç†ç»“æœ
            for i, future in enumerate(as_completed(future_to_image), 1):
                image = future_to_image[future]
                try:
                    success, message = future.result()
                    if success:
                        success_count += 1
                        print(f"âœ… ({i}/{len(images)}) {image['id']}")
                    else:
                        print(f"âŒ ({i}/{len(images)}) {image['id']}: {message}")
                except Exception as e:
                    print(f"âŒ ({i}/{len(images)}) {image['id']}: å¤„ç†å¼‚å¸¸ {e}")
        
        print(f"âœ… æ‰¹é‡ä¸Šä¼ å®Œæˆ: {success_count}/{len(images)} æˆåŠŸ")
        return success_count
    
    def sync_database_urls(self):
        """åŒæ­¥æ•°æ®åº“ä¸­çš„URL"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE images 
            SET url_regular = ? || '/images/' || id || '.png',
                url_download = ? || '/images/' || id || '.png'
            WHERE uploaded = TRUE
        """, (self.public_url, self.public_url))
        
        updated_count = cursor.rowcount
        conn.commit()
        conn.close()
        
        print(f"âœ… åŒæ­¥äº† {updated_count} æ¡æ•°æ®åº“è®°å½•çš„URL")
        return updated_count
    
    def get_upload_stats(self):
        """è·å–ä¸Šä¼ ç»Ÿè®¡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM images WHERE processed = TRUE")
        processed = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM images WHERE uploaded = TRUE")
        uploaded = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            'processed': processed,
            'uploaded': uploaded,
            'pending': processed - uploaded
        }

def main():
    parser = argparse.ArgumentParser(description="ä¸Šä¼ å›¾ç‰‡åˆ°R2å­˜å‚¨")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸Šä¼ ")
    parser.add_argument("--workers", type=int, default=5, help="å¹¶å‘ä¸Šä¼ æ•°")
    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºä¸Šä¼ ç»Ÿè®¡")
    parser.add_argument("--sync-urls", action="store_true", help="åŒæ­¥æ•°æ®åº“URL")
    
    args = parser.parse_args()
    
    uploader = R2Uploader()
    
    if args.stats:
        stats = uploader.get_upload_stats()
        print("ğŸ“Š ä¸Šä¼ ç»Ÿè®¡:")
        print(f"  å·²å¤„ç†: {stats['processed']}")
        print(f"  å·²ä¸Šä¼ : {stats['uploaded']}")
        print(f"  å¾…ä¸Šä¼ : {stats['pending']}")
    elif args.sync_urls:
        uploader.sync_database_urls()
    else:
        uploader.upload_batch(args.force, args.workers)

if __name__ == "__main__":
    main()